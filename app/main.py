from fastapi import FastAPI, Body
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

import os, time, json
import ollama
from httpx import ConnectError  

from app.ingest import ingest_data
from app.graph import run_once

import chromadb

# USE_STREAM_BACKEND = os.getenv("STREAM_BACKEND", "NONE").upper()  # "NONE" æˆ– "OLLAMA"
# OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.1:8b-instruct")         
USE_STREAM_BACKEND = "OLLAMA"  # "NONE" æˆ– "OLLAMA"
OLLAMA_MODEL = "llama3.2:3b"



app = FastAPI()

# ğŸ‘‡ åŠ å…¥é€™æ®µï¼ˆé–‹ç™¼éšæ®µå…ˆå…¨é–‹ï¼›ä¹‹å¾Œå¯æ”¹æˆç™½åå–®ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],        # æˆ–æ”¹æˆ ["http://localhost:8001", "http://127.0.0.1:8001"]
    allow_credentials=True,
    allow_methods=["*"],        # è®“ OPTIONS / POST éƒ½é€šé
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "Hello World"}

@app.post("/ingest")
def ingest():
    ingest_data()
    return {"ok": True}

# -------- Streaming SSE ----------
def gen_none_backend(q: str):
    """ä¸ä½¿ç”¨ä»»ä½•æ¨¡å‹ï¼Œç›´æ¥æŠŠ run_once çš„ answer åˆ‡ token åšå‡æµã€‚"""
    out = run_once(q, history=[])
    text = out.get("answer") or "(ç„¡å›ç­”)"
    yield "data: [thinking] æ­£åœ¨æª¢ç´¢çŸ¥è­˜åº«èˆ‡åŸ·è¡Œå·¥å…·...\n\n"
    for tok in text.split(" "):
        yield f"data: {tok}\n\n"
        time.sleep(0.02)
    yield "data: [meta] " + json.dumps(out, ensure_ascii=False) + "\n\n"
    yield "event: end\ndata: [DONE]\n\n"

def build_prompt_from_rag(q: str):
    """æŠŠ RAG è­‰æ“šèˆ‡å·¥å…·çµæœçµ„æˆæç¤ºè©ï¼Œæä¾›çµ¦ Ollamaã€‚"""
    out = run_once(q, history=[])
    ctx = "\n\n".join([f"[{c['source']}]\n{c['text']}" for c in out['contexts']]) or "(ç„¡æª¢ç´¢å‘½ä¸­)"
    tools = json.dumps(out['tool_results'], ensure_ascii=False)
    prompt = f"""ä½ æ˜¯åŠ å¯†äº¤æ˜“æ‰€å®¢æœåŠ©ç†ï¼Œè«‹ç”¨ç°¡æ½”ä¸­æ–‡å›ç­”ã€‚
                ä½¿ç”¨è¦å‰‡ï¼š
                - å…ˆçµ¦å‡ºç›´æ¥ç­”æ¡ˆï¼ˆè‹¥æœ‰ç™¾åˆ†æ¯”ï¼Œè«‹åŒæ™‚æä¾› 0.090% èˆ‡ 0.0009 é€™ç¨®å…©ç¨®å½¢å¼ï¼‰
                - å¦‚ç”¨åˆ°å·¥å…·æˆ–çŸ¥è­˜åº«ï¼Œçµå°¾åˆ—å‡ºä¾†æºæª”åï¼ˆä¸éœ€è¦æ®µè½ï¼‰
                - ä¸ç¢ºå®šå°±èªªç„¡æ³•ç¢ºå®šï¼Œä¸è¦èƒ¡ç·¨

                å•é¡Œï¼š{q}
                å·¥å…·çµæœï¼š{tools}
                å¯ç”¨è³‡æ–™ç‰‡æ®µï¼š
                {ctx}
                """
    return prompt, out  # å›å‚³ out ä¾¿æ–¼ meta é¡¯ç¤º

def gen_ollama_backend(q: str, model: str):
    import ollama, os
    from httpx import ConnectError
    os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
    os.environ["NO_PROXY"] = "127.0.0.1,localhost"

    host = os.getenv("OLLAMA_HOST")

    client = ollama.Client(host=host)


    prompt, meta_out = build_prompt_from_rag(q)
    yield f"data: [thinking] é€£ç·šåˆ° Ollamaï¼ˆ{host}ï¼‰ä¸¦ç”¢ç”Ÿå›ç­”...\n\n"
    try:
        for chunk in client.chat(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            stream=True
        ):
            msg = chunk.get("message", {})
            if isinstance(msg, dict) and msg.get("content"):
                yield "data: " + msg["content"] + "\n\n"
    except ConnectError:
        yield "data: [error] ç„¡æ³•é€£ç·šåˆ° Ollamaï¼Œè«‹ç¢ºèªå·²å•Ÿå‹• `ollama serve`ï¼Œä¸” OLLAMA_HOST æŒ‡å‘ http://127.0.0.1:11434ã€‚\n\n"
    yield "event: end\ndata: [DONE]\n\n"

@app.get("/stream")
def stream(q: str):
    if USE_STREAM_BACKEND == "OLLAMA":
        return StreamingResponse(gen_ollama_backend(q, OLLAMA_MODEL), media_type="text/event-stream")
    else:
        return StreamingResponse(gen_none_backend(q), media_type="text/event-stream")



@app.post("/chat")
def chat(payload: dict = Body(...)):
    q = payload.get("q", "")
    history = payload.get("history", [])
    out = run_once(q, history)
    return out



@app.get("/health")
def health():
    client = chromadb.PersistentClient(path="./index/chroma")
    db = client.get_collection("kb_main")
    return {
        "collection": "kb_main",
        "count": db.count(),
        "stream_backend": USE_STREAM_BACKEND,
        "ollama_model": OLLAMA_MODEL if USE_STREAM_BACKEND == "OLLAMA" else None
    }
