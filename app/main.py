from fastapi import FastAPI, Body
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

import os, time, json
import ollama
from httpx import ConnectError  

from app.ingest import ingest_data
from app.graph import run_once

import chromadb

# USE_STREAM_BACKEND = os.getenv("STREAM_BACKEND", "NONE").upper()  # "NONE" æˆ– "OLLAMA"
# OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.1:8b-instruct")         
USE_STREAM_BACKEND = "OLLAMA"  # "NONE" æˆ– "OLLAMA"
OLLAMA_MODEL = "llama3.2:3b"
os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
os.environ["NO_PROXY"] = "127.0.0.1,localhost"



app = FastAPI()

# ğŸ‘‡ åŠ å…¥é€™æ®µï¼ˆé–‹ç™¼éšæ®µå…ˆå…¨é–‹ï¼›ä¹‹å¾Œå¯æ”¹æˆç™½åå–®ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],        # æˆ–æ”¹æˆ ["http://localhost:8001", "http://127.0.0.1:8001"]
    allow_credentials=True,
    allow_methods=["*"],        # è®“ OPTIONS / POST éƒ½é€šé
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "Hello World"}

@app.post("/ingest")
def ingest():
    ingest_data()
    return {"ok": True}

# -------- Streaming SSE ----------
def gen_none_backend(q: str):
    import time, json
    # å…ˆå›å ±ï¼šå·²æ¥åˆ°è«‹æ±‚
    yield "data: [debug] stream start\n\n"

    # 1) é–‹å§‹ RAG
    yield "data: [debug] run_once:start\n\n"
    t0 = time.time()
    out = run_once(q, history=[])
    yield f"data: [debug] run_once:done ({time.time()-t0:.2f}s)\n\n"

    # 2) æ¨ç­”æ¡ˆï¼ˆå‡æµï¼‰
    text = out.get("answer") or "(ç„¡å›ç­”)"
    yield "data: [thinking] æ­£åœ¨è¼¸å‡º...\n\n"
    for tok in text.split(" "):
        yield f"data: {tok}\n\n"
        time.sleep(0.02)

    yield "data: [meta] " + json.dumps(out, ensure_ascii=False) + "\n\n"
    yield "event: end\ndata: [DONE]\n\n"

def build_prompt_from_rag(q: str):
    """æŠŠ RAG è­‰æ“šèˆ‡å·¥å…·çµæœçµ„æˆæç¤ºè©ï¼Œæä¾›çµ¦ Ollamaã€‚"""
    out = run_once(q, history=[])
    ctx = "\n\n".join([f"[{c['source']}]\n{c['text']}" for c in out['contexts']]) or "(ç„¡æª¢ç´¢å‘½ä¸­)"
    tools = json.dumps(out['tool_results'], ensure_ascii=False)
    prompt = f"""ä½ æ˜¯åŠ å¯†äº¤æ˜“æ‰€å®¢æœåŠ©ç†ï¼Œè«‹ç”¨ç°¡æ½”ä¸­æ–‡å›ç­”ã€‚
                ä½¿ç”¨è¦å‰‡ï¼š
                - å…ˆçµ¦å‡ºç›´æ¥ç­”æ¡ˆï¼ˆè‹¥æœ‰ç™¾åˆ†æ¯”ï¼Œè«‹åŒæ™‚æä¾› 0.090% èˆ‡ 0.0009 é€™ç¨®å…©ç¨®å½¢å¼ï¼‰
                - å¦‚ç”¨åˆ°å·¥å…·æˆ–çŸ¥è­˜åº«ï¼Œçµå°¾åˆ—å‡ºä¾†æºæª”åï¼ˆä¸éœ€è¦æ®µè½ï¼‰
                - ä¸ç¢ºå®šå°±èªªç„¡æ³•ç¢ºå®šï¼Œä¸è¦èƒ¡ç·¨

                å•é¡Œï¼š{q}
                å·¥å…·çµæœï¼š{tools}
                å¯ç”¨è³‡æ–™ç‰‡æ®µï¼š
                {ctx}
                """
    return prompt, out  # å›å‚³ out ä¾¿æ–¼ meta é¡¯ç¤º

# def gen_ollama_backend(q: str, model: str):
#     import ollama, os, json, time
#     from httpx import ConnectError
#     os.environ["OLLAMA_HOST"] = "http://127.0.0.1:11434"
#     os.environ["NO_PROXY"] = "127.0.0.1,localhost"

#     host = os.getenv("OLLAMA_HOST")

#     client = ollama.Client(host=host)


#     prompt, meta_out = build_prompt_from_rag(q)
#     yield f"data: [thinking] é€£ç·šåˆ° Ollamaï¼ˆ{host}ï¼‰ä¸¦ç”¢ç”Ÿå›ç­”...\n\n"
#     try:
#         for chunk in client.chat(
#             model=model,
#             messages=[{"role": "user", "content": prompt}],
#             stream=True
#         ):
#             msg = chunk.get("message", {})
#             if isinstance(msg, dict) and msg.get("content"):
#                 yield "data: " + msg["content"] + "\n\n"
#     except ConnectError:
#         yield "data: [error] ç„¡æ³•é€£ç·šåˆ° Ollamaï¼Œè«‹ç¢ºèªå·²å•Ÿå‹• `ollama serve`ï¼Œä¸” OLLAMA_HOST æŒ‡å‘ http://127.0.0.1:11434ã€‚\n\n"
#     yield "event: end\ndata: [DONE]\n\n"

def gen_ollama_backend(q: str, model: str):
    import ollama, os, json, time
    from httpx import ConnectError, ReadTimeout

    host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
    client = ollama.Client(host=host, timeout=120.0)

    # === 1) RAG å‰è™•ç† ===
    yield f"data: [debug] stream start (ollama={model} host={host})\n\n"
    yield "data: [debug] run_once:start\n\n"
    t0 = time.time()
    out = run_once(q, history=[])
    yield f"data: [debug] run_once:done ({time.time()-t0:.2f}s)\n\n"

    # çµ„ prompt
    yield "data: [debug] prompt:build\n\n"
    ctx = "\n\n".join([f"[{c['source']}]\n{c['text']}" for c in out['contexts']]) or "(ç„¡æª¢ç´¢å‘½ä¸­)"
    tools = json.dumps(out['tool_results'], ensure_ascii=False)
    prompt = f"""ä½ æ˜¯åŠ å¯†äº¤æ˜“æ‰€å®¢æœåŠ©ç†ï¼Œè«‹ç”¨ç°¡æ½”ä¸­æ–‡å›ç­”ã€‚
è‹¥æœ‰ç™¾åˆ†æ¯”ï¼Œæä¾› 0.090% èˆ‡ 0.0009 å…©ç¨®è¡¨ç¤ºï¼›çµå°¾åˆ—å‡ºä¾†æºæª”åã€‚
å•é¡Œï¼š{q}
å·¥å…·çµæœï¼š{tools}
å¯ç”¨è³‡æ–™ç‰‡æ®µï¼š
{ctx}
"""

    # === 2) å…ˆå˜—è©¦çœŸæ­£ä¸²æµ ===
    yield "data: [debug] ollama:generate:start\n\n"
    got_any = False
    last_ts = time.time()

    try:
        for chunk in client.generate(model=model, prompt=prompt, stream=True, options={"temperature": 0.2}):
            text = ""
            if isinstance(chunk, dict):
                # generate(stream=True) çš„éµ
                if chunk.get("response"):
                    text = chunk["response"]
                # å…¼å®¹ chat(stream=True) çš„éµï¼ˆä»¥é˜²ä½ æ”¹å› chatï¼‰
                elif isinstance(chunk.get("message"), dict):
                    text = chunk["message"].get("content", "")

            if text:
                if not got_any:
                    yield "data: [thinking] æ¨¡å‹å·²é–‹å§‹å›è¦†...\n\n"
                got_any = True
                yield "data: " + text + "\n\n"
                last_ts = time.time()

            # è‹¥ 3 ç§’æ²’ tokenï¼Œè¦–ç‚ºä¸²æµç•°å¸¸ â†’ è§¸ç™¼ fallback
            if time.time() - last_ts > 3.0 and not got_any:
                yield "data: [warn] 3s ç„¡ä¸²æµè¼¸å‡ºï¼Œå•Ÿç”¨ fallbackï¼ˆéä¸²æµç”Ÿæˆå†åˆ†æ®µè¼¸å‡ºï¼‰\n\n"
                raise TimeoutError("no_stream_tokens")

    except (TimeoutError, ReadTimeout):
        # === 3) Fallbackï¼šéä¸²æµç”Ÿæˆï¼Œæ‰‹å‹•åˆ‡ç‰‡åå‡º ===
        try:
            r = client.generate(model=model, prompt=prompt, stream=False, options={"temperature": 0.2})
            text = r.get("response", "") or ""
            if not text:
                yield "data: [error] fallback ä¹Ÿæ²’æœ‰å…§å®¹\n\n"
            else:
                yield "data: [thinking]ï¼ˆfallbackï¼‰\n\n"
                # ä½ å¯ä»¥æ”¹æˆé€å­—æˆ–é€å¥ï¼›é€™è£¡å…ˆç”¨ç©ºç™½åˆ‡
                for tok in text.split(" "):
                    yield "data: " + tok + "\n\n"
                    time.sleep(0.01)
        except Exception as e:
            yield "data: [error] fallback å¤±æ•—ï¼š" + f"{type(e).__name__}: {e}" + "\n\n"

    except ConnectError:
        yield "data: [error] ç„¡æ³•é€£ç·šåˆ° Ollamaï¼›è«‹ç¢ºèªå·²å•Ÿå‹• `ollama serve` ä¸¦è¨­å®š OLLAMA_HOSTã€‚\n\n"
    except Exception as e:
        yield "data: [error] " + f"{type(e).__name__}: {e}" + "\n\n"

    # === 4) é™„ä¸Š meta ä¾¿æ–¼é™¤éŒ¯ ===
    yield "data: [meta] " + json.dumps(out, ensure_ascii=False) + "\n\n"
    yield "event: end\ndata: [DONE]\n\n"



@app.get("/diag/ollama_gen")
def diag_ollama_gen():
    import os, ollama
    host = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
    client = ollama.Client(host=host)
    r = client.generate(model=os.getenv("OLLAMA_MODEL", "llama3.2:3b"), prompt="hi", stream=False)
    return {"host": host, "len": len(r.get("response","")), "preview": r.get("response","")[:50]}



@app.get("/stream")
def stream(q: str):
    if USE_STREAM_BACKEND == "OLLAMA":
        return StreamingResponse(gen_ollama_backend(q, OLLAMA_MODEL), media_type="text/event-stream")
    else:
        return StreamingResponse(gen_none_backend(q), media_type="text/event-stream")



@app.post("/chat")
def chat(payload: dict = Body(...)):
    q = payload.get("q", "")
    history = payload.get("history", [])
    out = run_once(q, history)
    return out



@app.get("/health")
def health():
    client = chromadb.PersistentClient(path="./index/chroma")
    db = client.get_collection("kb_main")
    return {
        "collection": "kb_main",
        "count": db.count(),
        "stream_backend": USE_STREAM_BACKEND,
        "ollama_model": OLLAMA_MODEL if USE_STREAM_BACKEND == "OLLAMA" else None
    }
